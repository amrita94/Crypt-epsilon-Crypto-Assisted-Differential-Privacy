\section{Background cntd}
\stitle{Differential Privacy Cntd}\begin{theorem}(Sequential Composition). If $\mu_1$ and
$\mu_2$ are $\epsilon_1$-DP and $\epsilon_2$-DP algorithms that use independent randomness, then releasing the outputs $\mu_1(D)$ and
$\mu_2(D)$ on database $D$ satisfies $\epsilon_1+\epsilon_2$-DP.\end{theorem} There exist advanced composition theorems that give tighter
bounds on privacy loss, but we
use sequential composition as defined above for our paper.

\stitle{Secure Computation cntd}
In this section we will be describing garbled circuits in more detail. The function $f$ is evaluated via a Boolean circuit.
 First the garbler associates  two random cryptographic keys $K_{wi}^0$ and $K^1_{wi}$ with each wire $w_i$ of the circuit such that they correspond to the bits $b_i=0$ and $b_i=1$ respectively. \eat{ Next for each binary gate with input wires $(w_i,w_j)$. Next, for each binary gate g (e.g., an OR-gate)
with input wires (wi, wj ) and output wire wk, the garbler
computes the four ciphertexts
Enc(Kbi wi ,Kbj
wj )
(Kg(bi,bj ) wk ) for bi, bj ∈ {0, 1} .
The set of these four randomly ordered ciphertexts defines
the garbled gate.
g bi
bj g(bi, bj ) = bi ∨ bj
K0
wi ,K1
wi
K0
wj ,K1
wj
K0
wk ,K1
wk
Figure 2: Example of a garbled OR-gate
It is required that the symmetric encryption algorithm
Enc, which is keyed by a pair of keys, has indistinguishable encryptions under chosen-plaintext attacks. It is also
required that given the pair of keys (Kbi
wi , Kbj
wj ), the corresponding decryption process unambiguously recovers the
value of Kg(bi,bj ) wk from the four ciphertexts constituting the
garbled gate; see e.g. [15] for an efficient implementation.
It is worth noting that the knowledge of (Kbi
wi , Kbj
wj ) yields
only the value of Kg(bi,bj ) wk and that no other output values
can be recovered for this gate. So the evaluator can evaluate
the entire garbled circuit gate-by-gate so that no additional
information leaks about intermediate computations.}


\stitle{Stability of a transformation}
\begin{definition}A transformation $\mathcal{T}$ is defined to be $t-$ stable if for two datasets $D$ and $D'$, we have\begin{gather}|\mathcal{T}(D)\ominus \mathcal{T}(D')| \leq t \cdot |D\ominus D'|  \end{gather} where $\ominus$ denotes the symmetric difference (i.e.,  $D \ominus D'$ is the set of records in $D$ or $D'$, but not both. \end{definition}
Transformations with bounded stability scale the differential privacy guarantee of their outputs, by their stability constant \cite{PINQ}.
\begin{theorem}
Let $\mu$ provide $\epsilon$-differential privacy, and let $\mathcal{T}$
be an arbitrary $t$-stable transformation. The composite computation $\mu \circ \mathcal{T}$ provides $(\epsilon \cdot t)$-differential privacy.\end{theorem}
\stitle{Noisy Max Cntd}
The Noisy Max algorithm has two fold advantage over the naive implementation of finding the maximum count.
Firstly, noisy-max applies "information minimization" as rather than releasing all the noisy counts
and allowing the analyst to find the max and its index, only the
index corresponding to the maximum is made public.
Secondly, the noise added is much smaller than that in the case of the naive implementation (it has sensitivity $m\cdot \Delta$).