\section{Introduction}
%There is a growing need for releasing aggregate properties from sensitive datasets in several domains  including social science, healthcare, and advertising. %Differentially private algorithms \cite{dwork}, whose outputs are insensitive to adding or removing a single row in the input dataset, have become the gold standard for these situations. Differential privacy provides a provable and persuasive guarantee of privacy to individuals in a dataset, and has seen adoption by government \cite{machanavajjhala08onthemap,Vilhuber17Proceedings} and commercial organizations \cite{Rappor1,Apple, Samsung}. %It is defined with respect to a privacy parameter $\epsilon > 0$ where lower the value of $\epsilon$, greater is the privacy guaranteed.
Differential privacy (DP) is a rigorous privacy definition that has become the gold standard for data privacy. %and has enjoyed adoption by both government \cite{machanavajjhala08onthemap,Vilhuber17Proceedings} and commercial organizations \cite{Rappor1,Apple, Samsung}.
It is typically implemented in one of two models -- \textit{centralized differential privacy} (\cdp) and \textit{local differential privacy} (\ldp). In \cdp, data from individuals are collected and stored \textit{in the clear} in a \textit{trusted} centralized data curator which then executes DP programs on the sensitive data  and releases outputs to an untrustworthy data analyst. %A canonical algorithm in \cdp is the Laplace mechanism where the curator releases the output of a function $f$ by adding noise drawn from the Laplace distribution to hide the presence or absence of one row in the input database. %Depending on infrastructural constraints, like the viable trust model in a given setting, differential private algorithms in practice can have two different styles of implementation.  The more common and historically precedent implementation is the central differential privacy (\textsf{CDP}) model where a trusted data curator collates data from all individuals into a centrally held dataset and processes it in a privacy preserving way. %For example, the data curator could publish differentially private statistics of the data, that allows analysis on the data, without revealing individual information. 
%The curator is trusted to store the data \emph{in the clear} and mediates upon queries posed by a mistrustful analyst, interested in learning some synopsis of this dataset. Privacy is enforced by the curator by adding uncertainty to the answers for analyst's queries before releasing them. The other competing model of implementation, is the local differential privacy model (\textsf{LDP}) where the central data aggregating server is untrusted. Thus every data owner has to individually randomize his/her input before communicating it to the central aggregator. Hence the private data is concealed from the untrusted aggregator who attempts to infer statistics about the true population from the perturbed data instead. 
In \ldp, there is no trusted data curator. Rather, each individual perturbs his/her own data using a (local) DP algorithm. The data analyst uses these noisy data to infer aggregate statistics of the datasets. In practice, \cdp's assumption of a trusted server is ill-suited for many applications as it constitutes a single point of failure for data breaches, and saddles the trusted curator with legal and ethical obligations to uphold data privacy. %For instance, Google Chrome uses the \ldp model rather than \cdp to detect changes in browser properties of its userbase as it does not want the legal burden of storing highly sensitive browser fingerprints in the clear on its servers \cite{Rappor1}. 
Hence recent commercial deployments of DP  \cite{Rappor1, Apple} have preferred  \ldp over \cdp. However, \ldp's attractive privacy properties come with a utilitarian price tag. %All DP algorithms ensure privacy by introducing noise into the computation. 
Under the \cdp model, the expected additive error for a aggregate count over a dataset of size $n$ is at most $\Theta(1/\epsilon)$ to achieve $\epsilon$-DP. %(e.g., using the Laplace mechanism \cite{dwork})%. 
In contrast, under the \ldp model, at least $\Omega(\sqrt{n}/\epsilon)$ additive expected error must be incurred by any $\epsilon$-DP program \cite{error1,error2,error3}, owing to the individual coin tosses of each data owner \cite{Prochlo,Rappor1,Rappor2,LDP1}. The \ldp model in fact imposes additional penalties on the algorithmic expressibility;  the power of \ldp is equivalent to that of the statistical query model \cite{SQ1} and there exists an exponential separation between the accuracy and sample complexity of \ldp and \cdp algorithms \cite{Kasivi}. 
% As a consequence, \ldp requires enormous amounts of data to obtain reliable population statistics. Unsurprisingly, only large corporations  like Google \cite{Rappor1,Rappor2,Prochlo} and Apple \cite{Apple} have attempted deploying \ldp.
 
\eat{  Specifically, computation in \textsf{LDP} results in an additional error of $\Omega(\sqrt{n})$ where $n$ is the total number of data owners contributing to the noisy estimate \cite{error1,error2,error3}. In contrast, we get a constant error bound for \textsf{CDP}.
For e.g., for a single counting query, in the \textsf{CDP} model, the trusted data curator first computes the true count in the clear. Next, adding a single instance of noise from the distribution $Lap(\frac{1}{\epsilon})$ to this true count guarantees $\epsilon$-differential privacy. Since the s.t.d of the distribution $Lap(\frac{1}{\epsilon})$ is given by $\frac{1}{\epsilon}$, we get an expected error of $O(\frac{1}{\epsilon})$. On the other hand, owing to the independent coin flips of each reporting data owner, the resulting noise in \textsf{LDP} induces a binomial distribution. This binomial distribution can be approximated by a normal distribution; the magnitude of this random Gaussian
noise however can be very large, its
standard deviation grows in proportion to the square root of
the report count $ \Omega\big(\frac{\sqrt{n}}{\epsilon}\big)$, and the noise is in practice higher by an
order of magnitude \cite{Prochlo,Rappor1,Rappor2,LDP1}. %often growing with the data dimension

 Thus, if a billion individuals'
reports are analyzed, then a common signal from even
up to a million reports may be missed. 


The construct of the \textsf{LDP} model in fact imposes additional penalties in terms of the  algorithmic expressibility.  Kasiviswanathan et al. in \cite{Kasivi} showed that the power of \textsf{LDP} is equivalent to that of the statistical query model \cite{SQ1} from learning theory and there exists an exponential separation between the accuracy and sample complexity of local and central algorithms.  As a consequence, \textsf{LDP} requires enormous amounts of data \cite{Kasivi}
to obtain reliable population statistics. Unsurprisingly, only large corporations  like Google \cite{Rappor1,Rappor2,Prochlo} and Apple \cite{Apple}, with  billions of user base have had successful commercial deployment of \textsf{LDP}.
} %More abstractly, the power of the local model is equivalent tothe statistical query model from learning theory and t.

 
In this paper, we strive to bridge the gap between \textsf{LDP} and \textsf{CDP}. We propose, \system, a system and a programming framework for executing DP programs that: 
\squishlist
\item never stores or computes on sensitive data in the clear,
and still
\item achieves the accuracy guarantees and algorithmic expressibility of the \cdp model \squishend 
\system employs a pair of untrusted but non-colluding servers -- Analytics Server (\textsf{AS}) and  Cryptographic Service Provider (\textsf{CSP}). The \textsf{AS} executes DP programs (like the data curator in \cdp) but on \textit{encrypted} data records. The \textsf{CSP} initializes and manages the cryptographic primitives, and collaborates with the \textsf{AS} to generate the program outputs. Under the assumption that the \textsf{AS} and the \textsf{CSP} are semi-honest and do not collude (a common assumption in cryptographic systems \cite{Boneh1,Boneh2,Ridge2,Matrix2,secureML,LReg,Ver}), \system ensures $\epsilon$-DP guarantee for its programs via two cryptographic primitives -- linear homomorphic encryption (LHE) and garbled circuits. 

 \system provides a data analyst with a programming framework to author logical DP programs just like in the \cdp model.  Like in prior work \cite{PINQ, FWPINQ, ektelo}, access to the sensitive data is restricted via a set of predefined transformations operators (inspired by relational algebra) and DP measurement operators (Laplace mechanism and Noisy-Max \cite{Dork}). Thus, any program that is expressed as a composition of the above operators automatically satisfies $\epsilon$-DP (in \cdp model) giving the analyst a proof of privacy for free. 
 \system programs support constructs like looping and conditionals, and can arbitrarily post-process outputs of measurement operators.  \\The main contributions of this work are
\squishlist
\item \textbf{New Approach:} We present the design and implementation of \system, a novel system and programming framework for executing DP programs over encrypted data on two non-colluding untrusted servers. %\system integrates the constant error bounds of \textsf{CDP} with the low trust assumption of \textsf{LDP}.
\item \textbf{Algorithm Expressibility:} \system supports a rich class of state-of -the-art DP programs expressed in terms of a small set of transformation and measurement operators. Thus, \system achieves the accuracy guarantees of the \cdp model without the need for a trusted curator.  
\item \textbf{Ease Of Use:} \system abstracts out all the low-level implementation details %like the choice of input data representation, SMC protocol, key management scheme etc 
from the data analyst thereby reducing his/her burden of complex decision making. The data analysts need to encode only the DP program logic in terms of the \system operators. \system automatically compiles this down to the underlying implementation and provides a DP guarantee (in the \cdp model) for free. 
\item \textbf{Performance Optimizations:} %Existing techniques to compile logical \system programs into cryptographic protocols often result in inefficient programs.
We propose optimizations that speed up computation on encrypted data by at least an order of magnitude. A novel contribution of this work is DP indexing optimization that leverages on the fact that the final output being noisy, the mechanism can tolerate DP leakage of certain intermediate statistics. 
\item \textbf{Practical for Real World Usage:} %We demonstrate the accuracy and efficiency of \system via extensive  evaluation. 
For the same tasks, \system programs achieve accuracy comparable to \textsf{CDP} which is orders of magnitude (at least 2) more than that of \textsf{LDP}. \system runs within 5 min for a large class of programs on a dataset with 32,561 rows and 4 attributes. 
\item \textbf{Generalized Multiplication Using \textsf{LHE}:} Our implementation uses an efficient way for performing $n$-way multiplications using \textsf{LHE} which maybe of independent interest.
\squishend
 
  

