\section{Introduction}

\eat{\am{Outline of the intro: 
\begin{itemize}
	\item Para 1: Consensus on DP/informal definition
	\item Para 2 :Two models of implementation -- central vs local. Informally define the models. Not sure if you need to go into Laplace random variables or binomial random variables. 
	\item Para 3: LDP is popular in some deployments due to security ... 
	\item Para 4: Utility cost: use lower bounds rather than upper bound results for LDP. I see a $O()$ ... should use $\Omega()$. A better way to do this would be to first discuss the lower bounds, and then give an example of a simple count query and show how the separation works. In particular, you can answer a count query using Laplace mechanism in the central model with $O(1/\epsilon)$, but you need $\Omega(\sqrt{n}/\epsilon)$ noise for the LDP model. Read the Adam smith/Ulfar papers on the anonymous channels to see how this is written in other papers. 
	\item Para 5: In this paper we present a system that uses cryptography to bring the gap ...: See how I wrote the abstract and rewrite this para. Say one line about how this is in line with a growing line of literature that is using crypto to address the trust assumptions in central model [citations] (see related work section for details). Also, when you first say two-server model, give a bunch of citations to the crypto-assisted model. 
	\item Contributions: see comments later on.   
\end{itemize}}}

%Thre has been a growing consensus to papers like Contributions of our work is three fold%$\begin{enumerate}\item we have by achieving the \end{enumerate}
%Differential privacy has steadily emerged as a compelling privacy definition in situations when aggregate statistics must be released from databases without revealing properties of individual records. %that allows us to glean useful information about data alongside providing privacy at the granularity of individuals. %Formal privacy guarantees in tandem with quantifiable utility-privacy trade-offs have been instrumental in making it the de-facto standard for achieving data privacy.
%An algorithm satisfies differential privacy if adding or removing a single record from its input does not significantly change its output \cite{dwork}. This is a compelling guarantee as it bounds the additional privacy risk to any individual record  % It is essentially a property of a function such that its output should disallow any inference about the presence (or absence equivalently) of any individual record in its input data set \cite{dwork}. Formally, it requires that for any outcome of a randomized function, that outcome should be nearly equally likely with and without any one record. 

In several domains, including social science, healthcare, and advertising, there is growing need for releasing aggregate properties from sensitive datasets. Differentially private algorithms \cite{dwork}, whose outputs are insensitive to adding or removing a single row in the input dataset, have arisen as the gold standard for these situations. Differential privacy provides a provable and persuasive guarantee of privacy to individuals in a dataset, and has seen adoption in government \cite{machanavajjhala08onthemap,Vilhuber17Proceedings} and commercial organizations \cite{Rappor1,Apple, Samsung}. It is defined with respect to a privacy parameter $\epsilon > 0$ where lower the value of $\epsilon$, greater is the privacy guarantee obtained.

Differential privacy (DP) is typically implemented in one of two models -- \textit{centralized differential privacy}, or \cdp, and \textit{local differential privacy}, or \ldp. In \cdp, data from individuals are collected and stored \textit{in the clear} in a \textit{trusted} centralized data curator. The trusted curator executes DP programs on the sensitive data  and releases outputs to a mistrustful data analyst. A canonical algorithm in \cdp is the Laplace mechanism where the curator releases the output of a function $f$ by adding noise drawn from the Laplace distribution to hide the presence or absence of one row in the input database. 

%Depending on infrastructural constraints, like the viable trust model in a given setting, differential private algorithms in practice can have two different styles of implementation.  The more common and historically precedent implementation is the central differential privacy (\textsf{CDP}) model where a trusted data curator collates data from all individuals into a centrally held dataset and processes it in a privacy preserving way. %For example, the data curator could publish differentially private statistics of the data, that allows analysis on the data, without revealing individual information. 
%The curator is trusted to store the data \emph{in the clear} and mediates upon queries posed by a mistrustful analyst, interested in learning some synopsis of this dataset. Privacy is enforced by the curator by adding uncertainty to the answers for analyst's queries before releasing them. The other competing model of implementation, is the local differential privacy model (\textsf{LDP}) where the central data aggregating server is untrusted. Thus every data owner has to individually randomize his/her input before communicating it to the central aggregator. Hence the private data is concealed from the untrusted aggregator who attempts to infer statistics about the true population from the perturbed data instead. 

In \ldp, there is no trusted centralized data curator. Rather, each individual perturbs their own data using a (local) differentially private algorithm. The data analyst has direct access to these perturbed data, and uses it to infer aggregate statistics of the datasets. A canonical \ldp algorithm is the randomized response mechanism \cite{RR} where each data owner flips some of his/her input bits based on a coin toss to provide \emph{plausible deniability} \cite{Dork}.


Recent deployments of DP in commercial organizations \cite{Rappor1, Apple} have preferred the \ldp model over the \cdp model. \cdp's assumption of a trusted server that stores data in the clear is ill-suited for many applications as it constitutes a single point for data breaches, and saddles the trusted curator with legal and ethical obligations to keep the user data secure. For instance, Google Chrome uses the \ldp model rather than \cdp to detect changes in browser properties of its userbase as it does not want the legal burden of storing highly sensitive browser fingerprints in the clear on its servers \cite{Rappor1}. 
  

However, \ldp's attractive security properties come with a utilitarian price tag. All DP algorithms ensure privacy by introducing noise into the computation. Under the \cdp model, one can release an aggregate count over the dataset having $n$ rows with an expected additive error of at most $\Theta(1/\epsilon)$ and ensure $\epsilon$-DP (e.g., using the Laplace mechanism \cite{dwork}). In contrast, under the \ldp model, at least $\Omega(\sqrt{n}/\epsilon)$ additive error in expectation must be incurred by any $\epsilon$-DP program for this task \cite{error1,error2,error3}. This is because, each individual perturbs their data values independently in the \ldp model, contributing to the increased noise \cite{Prochlo,Rappor1,Rappor2,LDP1}. Thus, on a dataset with a billion individuals, properties that are common to a population as large as 30,000 individuals can be missed under \ldp. 

The \ldp model imposes additional penalties in terms of the  algorithmic expressibility.  Kasiviswanathan et al. in \cite{Kasivi} showed that the power of \ldp is equivalent to that of the statistical query model \cite{SQ1} from learning theory and there exists an exponential separation between the accuracy and sample complexity of local and centralized DP algorithms.  As a consequence, \ldp requires enormous amounts of data to obtain reliable population statistics. Unsurprisingly, only large corporations  like Google \cite{Rappor1,Rappor2,Prochlo} and Apple \cite{Apple} have attempted deploying \ldp.
 
\eat{  Specifically, computation in \textsf{LDP} results in an additional error of $\Omega(\sqrt{n})$ where $n$ is the total number of data owners contributing to the noisy estimate \cite{error1,error2,error3}. In contrast, we get a constant error bound for \textsf{CDP}.
For e.g., for a single counting query, in the \textsf{CDP} model, the trusted data curator first computes the true count in the clear. Next, adding a single instance of noise from the distribution $Lap(\frac{1}{\epsilon})$ to this true count guarantees $\epsilon$-differential privacy. Since the s.t.d of the distribution $Lap(\frac{1}{\epsilon})$ is given by $\frac{1}{\epsilon}$, we get an expected error of $O(\frac{1}{\epsilon})$. On the other hand, owing to the independent coin flips of each reporting data owner, the resulting noise in \textsf{LDP} induces a binomial distribution. This binomial distribution can be approximated by a normal distribution; the magnitude of this random Gaussian
noise however can be very large, its
standard deviation grows in proportion to the square root of
the report count $ \Omega\big(\frac{\sqrt{n}}{\epsilon}\big)$, and the noise is in practice higher by an
order of magnitude \cite{Prochlo,Rappor1,Rappor2,LDP1}. %often growing with the data dimension

 Thus, if a billion individuals'
reports are analyzed, then a common signal from even
up to a million reports may be missed. 


The construct of the \textsf{LDP} model in fact imposes additional penalties in terms of the  algorithmic expressibility.  Kasiviswanathan et al. in \cite{Kasivi} showed that the power of \textsf{LDP} is equivalent to that of the statistical query model \cite{SQ1} from learning theory and there exists an exponential separation between the accuracy and sample complexity of local and central algorithms.  As a consequence, \textsf{LDP} requires enormous amounts of data \cite{Kasivi}
to obtain reliable population statistics. Unsurprisingly, only large corporations  like Google \cite{Rappor1,Rappor2,Prochlo} and Apple \cite{Apple}, with  billions of user base have had successful commercial deployment of \textsf{LDP}.
} %More abstractly, the power of the local model is equivalent tothe statistical query model from learning theory and t.


In this paper, we bridge the gap between \ldp and \cdp. We propose, \system, a new model for differential privacy that: 
\squishlist
\item never stores or computes on sensitive data in the clear, and yet
\item achieves the accuracy guarantees and algorithmic expressibility of the \cdp model \squishend 
In \system a single trusted data curator is replaced by a pair of untrusted but non-colluding servers -- the Analytics Server (\textsf{AS}) and the Cryptographic Service Provider (\textsf{CSP}). The \textsf{AS} plays the role of  data curator in \cdp in executing the differentially private programs, but on \textit{encrypted} data records. The \textsf{CSP} initializes and manages the cryptographic primitives, and collaborates with the \textsf{AS} when the DP program needs to generate outputs. Under the assumption that the \textsf{AS} and the \textsf{CSP} are semi-honest and do not collude (a common assumption in the cryptography-assisted computation literature \cite{Boneh1,Boneh2,Ridge2,Matrix2,secureML,LReg,Ver}), \system is designed to reveal no extra information beyond what can be learned by the release of the outputs of a differentially private program. This is achieved using cryptographic primitives like linear homomorphic encryption and Yao's garbled circuits. 

\system permits analysts to author logical programs just like in the \cdp model. Like in prior work \cite{PINQ, FWPINQ, ektelo}, every logical program is guaranteed to satisfy differential privacy (in the \cdp model) by restricting access to the sensitive data via data transformations operators (inspired by relational algebra) and differentially private measurement operators ( Laplace mechanism and Noisy-Max \cite{Dork}). Programs can have constructs like looping and conditionals, and can arbitrarily post-process outputs of measurement operators. Such logical programs have been shown to express state-of-the art DP algorithms under the \cdp model. \system compiles these logical programs into \system protocols that can work on the encrypted data on the \textsf{AS} and \textsf{CSP}. 


The main contributions of this work are
\squishlist
\item \textbf{New Approach:} We present the design and implementation of \system, a novel system for executing differentially private programs over encrypted data on two non-colluding untrusted servers. %\system integrates the constant error bounds of \textsf{CDP} with the low trust assumption of \textsf{LDP}.
\item \textbf{Algorithm Expressivity:} \system supports data analysis using a rich class of state of the art differentially private programs expressed in terms of a small set of transformation and measurement operators. Thus, \system achieves the accuracy guarantees of the \cdp model without the need for a trusted curator.  
\item \textbf{Performance Optimizations:} Existing techniques to compile logical \system programs into crypto protocols often result in programs that are inefficient. We present optimizations that speed up computation on encrypted data by at least an order of magnitude. A novel contribution of this work are DP indexing optimizations that leverage the fact that differentially private programs can reveal statistical information about the data. 
\item \textbf{Practical for Real World Usage} We demonstrate the accuracy and practical efficiency of \system via extensive  evaluation. For the same tasks, \system programs achieve accuracy comparable to \textsf{CDP} and orders of magnitude (at least 2) more accuracy than that of \textsf{LDP}. Moreover, \system is efficient and runs within 5 min for a large class of programs on a dataset with 32,561 rows and 4 attributes. 
\item \textbf{Generalized Multiplication Using Linear Homomorphic encryption}- Our implementations leverage a novel efficient method of performing $n$-way multiplications using linear homomorphic encryptions, which maybe of independent interest.
\squishend


\noindent\textbf{Organization:} We introduce differential privacy and cryptographic primitives used in \system in Section~\ref{sec:background}. We present the system overview in Section~\ref{sec:overview}. \system primitives and their implementation are outlined in Sections~\ref{sec:primitives} and \ref{sec:implementaion}, respectively.  Optimizations to \system are described in Section~\ref{sec:optimization}. \system is empirically evaluated in Section~\ref{sec:experiments}. Related work and conclusions are discussed in Sections~\ref{sec:related-short} and \ref{sec:conclusions}, respectively.

\eat{Users (or data owners) encrypt their data using public keys generated by the CSP, and the encrypted data are stored on the \textsf{AS}. Using linear homomorphic encryption and Yao's garbled circuits, the AS and CSP collaboratively execute  \cite{Boneh1,Boneh2,Ridge2,Matrix2,secureML,LReg,Ver}.

The relaxation of not having trusted servers is effectuated by the use of cryptographic primitives, specifically linear homomorphic encryption and Yao's garbled circuits. The \textsf{AS} plays a role akin to that of the data curator in \textsf{CDP} or the data aggregator in \textsf{LDP} while the \textsf{CSP} initializes and manages the cryptographic primtives in \system. \system enables the computation of a rich class of differentially private algorithms by executing programs expressed as a sequence of a small set \system primitives inspired by relational algebra. At the very outset, the data owners submit their encrypted data to the \textsf{AS} which collates all the data records into a single encrypted database.  On receiving a data query from an external analyst expressed in the form of a \system program, the \textsf{AS} executes it on the encrypted database via some interactions with the \textsf{CSP}.  Crypt$\epsilon$ is designed to reveal no extra information (beside that released by the differentially private outputs itself) to
the two servers under the assumption of  non-collusion (this condition, for e.g., can be enforced by law).  Our work is in tune with a growing direction of research that seeks to address the trust assumptions of \textsf{CDP} via assistance from cryptographic techniques \cite{Prochlo,mixnets,amplification,Shi,Shi2,kamara,Rastogi}.
%Our solution is  based on linearly-homomorphic
%encryption (LHE) scheme and two instances of two-party Yao's garbled circuit; both these primitives can be implemented in practice via very efficient constructions. Additionally we propose two differentially private optimizations for Crypt$\epsilon$ that provide substantial performance gain over the base case implementation.
}

%\par The rest of the paper is organized as follows. The next section provides a brief discourse on the necessary background followed by the description of the Crypt$\epsilon$ system in section 3. In section 4 we talk about the functionality and implementation of Crypt$\epsilon$ primitives. Section 5 includes running examples of Crypt$\epsilon$ programs while section 6 proposes two optimizations. We empirically evaluate \system and  review the existing literature in sections 7 and 8 respectively. In section 9 we discuss about some interesting aspects of Crypt$\epsilon$ and finally conclude with future research directions in section 10. 



