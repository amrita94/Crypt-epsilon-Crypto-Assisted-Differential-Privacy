\section{\system Optimizations}\label{sec:optimization}
In this section we present several optimization techniques used by \system. These optimizations preserve differential privacy.
\subsection*{Differential Privacy Based Optimizations}
These optimizations leverage on the fact that the final output is  differentially private and expend the privacy budget.\\
(1)\stitle{ DP Indexing} \\
This optimization is motivated by the fact that the number of records needed by many programs is much smaller than the total dataset size.  For example, the program  P5 in Table~\ref{tab:programexamples} only needs records that have $Age=30$. Hence, we create an index for the database on some attribute of choice $A$. There are two heuristics that can be considered for selecting this indexing attribute $A$. First, $A$ should be very frequently queried. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Second, if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,i}, i \in [n] << m$ where $ct_{A,i}$ is the number of records in $\encD$ having value $v_i$ for attribute $A$. This would ensure that the first selection performed alone on $A$ will filter out majority of the records and reduce the intermediate dataset size to be considered for the subsequent predicate. 

%\\\textbf{1. Index} - Consider a conjunctive query predicate $A_1==v_1 \wedge A_2==v_2 \wedge A_3==v_3 $. If it so happens that the number of records satisfying $A_1==v_1$ is very low as compared to the total number of records, i.e., $ct_{A_1,v_1} << m$, then if a selection operation is performed alone on attribute $A_1$ then the size of the dataset to be considered for the subsequent clause $A_2==v_2 \wedge A_3==v_3$ reduces to only $ct_{A_1,v_1}$ as opposed to the whole dataset (size $m$). Our index optimization leverages on this idea - we create an index by sorting the database on an attribute of choice $A$. There are two heuristics that can be considered for selecting this indexing attribute $A$ - firstly $A$ should be a very frequently queried upon attribute. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Secondly if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,v_i}, i \in [n] << m$. This would ensure that the first selection operator performed alone on $A$ will filter out majority of the records and reduce the  dataset size to be considered for the subsequent predicate. 

Given an encrypted database $\encD$ and an attribute $A$ of $\encD$, we first use a garbled circuit to sort $\encD$ over $A$ into $\encD_s$ and then build a differentially private indexing on the sorted database $\encD_s$. The details are provided in Appendix~\ref{index-imp}. Next we build the index on $A$ with a privacy budget $\epsilon_{A}$ as follows. Let $P=(P_1,\ldots,P_k)$ be a uniform partition on the sorted domain of $A$ such that each partition contains $\frac{s_A}{k}$ consecutive domain values. We compute a noisy CDF $\hat{\mathcal{C}}$ vector $\hat{V}$ of length $k$ such that $\hat{V}[i]=\sum_j ct_{A,j}+\eta_i$ where $i \in [k], j \in P_i$ and $\eta_i\sim Lap(1/\epsilon_A)$. 
Next the \textsf{AS} computes a noisy CDF, $\hat{\mathcal{C}}$ over the $k$ bins using the noisy counts in $\hat{V}$ using inference based on isotonic regression \cite{cdf}. For executing a program conditioned as $\phi=A \in [v_s,v_e]$, we compute first compute $i_{start}=\hat{\mathcal{C}}[r_s-1], v_s \in [\frac{s_A}{k}\cdot(r_s-1)+1,\frac{s_A}{k}\cdot r_s]$, i.e., $v_s$ belongs to bin $r_s$   and  $i_{end}=\hat{\mathcal{C}}[r_e], v_e \in [\frac{s_A}{k}\cdot(r_e-1)+1,\frac{s_A}{k}\cdot r_e]$, i.e., $v_e$ belongs to bin $r_e$. We then run the program on this subset of records in $[i_{start},i_{end}]$. For increased accuracy we can also consider preceding bins of $r_s-1$ for $i_{start}$ and succeeding bins of $r_e$ for $i_{end}$.\\
\textbf{Optimized feature} - This optimization speeds up the execution time by reducing the number of total records to be processed for the program execution.\\
\textbf{Trade-off} - The trade-off is  a possible increase in error as the noisy selection of records from the index might miss some of the records that do satisfy the filter condition.\\
\textbf{Privacy Cost} - Assuming the index is constructed with a privacy parameter $\epsilon_A$, a selection of a subset of records using it will result in a $\epsilon_A$ - DP intermediate result. If $\epsilon$ is the parameter used for the subsequent measurement primitives, then by Theorem 1, the total privacy parameter is $\epsilon_A+\epsilon$.\\
\begin{comment}
For answering queries of the form $\phi=A_1==v_1\wedge \ldots  \wedge A_n==v_n$, ideally we just need to compute for $A_2==v_2\wedge \ldots \wedge A_n==v_n$ on $ct_{A,v}$ number of records starting from position $\sum_{i=1}^{i=v-1}ct_{A,i}$ of $\boldsymbol{\mathcal{\tilde{D}}}_{sort}$. 

However the \textsf{AS} has access only to the noisy CDF over the $k$ bins $ct_{A,i}$. Note that when $\bar{i}_{start}=\bar{\hat{\mathcal{C}}}[v-1] < \sum_{i=1}^{i=v-1}ct_{A,i}$ and $\bar{i}_{end}= \bar{\hat{\mathcal{C}}}[v-1] > i_{start}+ct_{A,v}$, i.e., the indices computed from the noisy values  saddle over the true records satisfying $A==v$, then although we end up loosing in performance a bit, we are still guaranteed to compute the exact non-noisy count for records satisfying $\phi$. 

In all other cases, we end up disregarding some of the records that satisfy $A==v$, some of these rejected records in fact might additionally satisfy $A_1==v_1 \wedge \ldots \wedge A_n==v_n$. Thus we might get inaccurate answer for query predicate $\phi$ (note that here we are talking about the encrypted true count of the given query predicate that is computed by the AS via a series of transformations before applying the LaplaceMechanism primitive).  An effective heuristic to tackle this can be to compensate for the expected laplacian error as follows  $\bar{i}_{start}= \bar{\hat{\mathcal{C}}}[v-1]-\frac{2}{\epsilon}$ and $\bar{i}_{end}=\bar{\hat{\mathcal{C}}}[v]+\frac{2}{\epsilon}$. Also note that answering differentially private  range queries   on attribute $A$ can also be directly done from the noisy CDF $\bar{\hat{\mathcal{C}}}$ 
\end{comment}
 (2)\stitle{ DP Range Tree}\\
 Range queries constitute a very popular category of queries for typical databases and range trees are a popular data structure constructed to speed up range query answering. A 1-dimensional range tree for an attribute $A$ is an ordered data structure such that the leaf nodes correspond to the individual counts $ct_{A,i}$, $v_i \in dom(A)$ while each parent node stores the sum of the counts of its children. Hence an useful optimization for \system can be pre-computing the noisy range tree for some of the most popular attributes. This can be useful for programs P1 and P2 in Table 3. %In Crypt$\epsilon$ we construct a noisy range tree for some of the attribute.
 The sensitivity for each such noisy range tree is $\log k$ where $k$ is the domain size of the attribute on which the tree is constructed. For answering any arbitrary range query, we need to access at most $2\log k$ nodes of the range tree. Thus to answer all possible range queries for the given attribute, the total squared error accumulated is $O(\frac{k^2(\log k)^2 }{\epsilon})$. In contrast for the naive case, we would have incurred error $O(\frac{k^3}{\epsilon})$. Hence this range tree optimization not only gives us a huge performance boost but also results in better answer accuracy. \\
\textbf{Optimized Feature} - The DP range tree optimization not only reduces the execution time, but also the expected error when executed over multiple range queries.\\\textbf{Trade-off} - The trade-off for this optimization is its storage cost $(O(2\cdot s_A))$.\\\textbf{Privacy Cost} - If the range tree is constructed  with privacy parameter $\epsilon_R$ then any measurement on it is $\epsilon_R$-DP.
\subsection*{Implementation Based Optimizations}
These optimizations work completely on the encrypted data and do not expend any privacy budget. Hence they do not affect the accuracy of the executed programs.\\
(1)\stitle{ Precomputation}  \\
Recall that the data owners $\textsf{DO}_i$ send per-attribute encrypted one-hot-codings of their data. We can use the \textsf{CrossProduct} primitive to generate the one-hot-coding of data across two attributes. However due to the intermediate interactions required with the \textsf{CSP}, the \textsf{CrossProduct} is a very costly operation. Hence an  useful optimization can be pre-computing the one-hot-coding for the data across a set of popular attributes $A^* \subset \mathcal{A}$ so that during subsequent program executions, the \textsf{AS} can get the desired representation via simple look-ups.  This can be useful for example P3 in Table 3.\\
\textbf{Optimized Feature} - This optimization improves the execution time of \system programs. Moreover, once the multi-attribute one-hot-codings are created it can be re-used for all subsequent programs.\\
\textbf{Trade-off} - The trade-off for this pre-computation is the storage cost (O($s_{A^*}=\prod_{A \in A^*}s_A$)) incurred to store the multi-attribute one-hot-codings for $A^*$.\\
\textbf{Privacy Cost} - Since this computation is carried completely on the encrypted data, we do not expend any privacy budget.\\
(2)\stitle{ Off-line Processing}\\
Recall that the in the implementation of $\textsf{GroupByCount}$ primitive, the \textsf{CSP} needs to generate the encrypted one-hot-codings for the masked histogram (step 5 in Algorithm 2). But the one-hot-coding representation for any such count would simply be a vector of $m-1$ $labEnc_{pk}(0)$ ciphers and 1 $labEnc_{pk}(1)$ cipher. Thus one useful optimization can be generating these ciphers for $0$ and $1$ in an off-line phase. (This is similar to the idea of off-line generation of Beaver's multiplication triples \cite{Beaver} used in secure multi-party computation.) In this way the encryption time will not be a part of the program execution time thereby giving us a performance boost.\\
\textbf{Optimized Feature} - This optimization results in a reduction in the run time of \system programs. \\
\textbf{Trade-off} - The trade-off for this optimization is the storage cost (O($m\cdot s_A$)) incurred to store the ciphers for attribute $A$.\\
\textbf{Privacy Cost} - The computation is carried completely on the encrypted data, we do not expend any privacy budget.
%\paragraph*{\textbf{Optimized Crypt$\epsilon$ programs}}
%Let us reconsider the example programs covered in section 5. Both Program 1 and Program 2 can be optimized by constructing a range tree over attribute $Age$. Program  4 and Program 5  on the other hand can be improved by the differentially private index over attribute $NativeCountry$ while for Program 6 we can create the index over attribute $Gender$.

