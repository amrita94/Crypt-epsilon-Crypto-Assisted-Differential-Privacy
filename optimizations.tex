\section{Optimization}\label{sec:optimization}
In this section we present several optimization techniques used by \system. Even with these techniques, we guarantee that the overall privacy loss is differentially private.
%optimizations that leverages on the fact that the analyst is only interested in a  differentially private view of our private database. These DP based optimizations help in reducing the performance cost significantly. %which would have not been possible with just secure multi-party computation.

\subsection{DP Indexing} 
This optimization is motivated by the fact that the number of records needed for many programs is much smaller than the total dataset size.  For example, the program  P5 in Table~\ref{tab:programexamples} only needs records that have $Age=30$. Hence, we create an index for the database on some attribute of choice $A$. There are two heuristics that can be considered for selecting this indexing attribute $A$. First, $A$ should be a very frequently queried upon attribute. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Second, if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,i}, i \in [n] << m$. This would ensure that the first selection performed alone on $A$ will filter out majority of the records and reduce the intermediate dataset size to be considered for the subsequent predicate. 

%\\\textbf{1. Index} - Consider a conjunctive query predicate $A_1==v_1 \wedge A_2==v_2 \wedge A_3==v_3 $. If it so happens that the number of records satisfying $A_1==v_1$ is very low as compared to the total number of records, i.e., $ct_{A_1,v_1} << m$, then if a selection operation is performed alone on attribute $A_1$ then the size of the dataset to be considered for the subsequent clause $A_2==v_2 \wedge A_3==v_3$ reduces to only $ct_{A_1,v_1}$ as opposed to the whole dataset (size $m$). Our index optimization leverages on this idea - we create an index by sorting the database on an attribute of choice $A$. There are two heuristics that can be considered for selecting this indexing attribute $A$ - firstly $A$ should be a very frequently queried upon attribute. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Secondly if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,v_i}, i \in [n] << m$. This would ensure that the first selection operator performed alone on $A$ will filter out majority of the records and reduce the  dataset size to be considered for the subsequent predicate. 

Given an encrypted database $\encD$ and an attribute $A$ of $\encD$, we first use a garbled circuit to sort $\encD$ over $A$ into $\encD_s$ and then build a differentially private indexing on the sorted database $\encD_s$. The sorting details are provided in Appendix~\ref{index-imp}. Next we build the index on $A$ given a privacy budget $\epsilon_{A}$ as follows. Let $P=(P_1,\ldots,P_k)$ be a uniform partition on the sorted domain of $A$ such that each partition contains $\frac{s_A}{k}$ consecutive domain values. We compute a $k$ lengthed vector $\hat{V}$ such that $\hat{V}[i]=\sum_j ct_{A,j}+\eta_i$ where $i \in [k], j \in P_i$ and $\eta_i\sim Lap(1/\epsilon_A)$. 
Next the \textsf{AS} computes a noisy CDF, $\hat{\mathcal{C}}$ over the $k$ bins using the noisy counts in $\hat{V}$ using inference based on isotonic regression \cite{cdf}. For executing a program conditioned as $\phi=A \in [v_s,v_e]$, we compute first compute $i_{start}=C[r_s-1], v_s \in [\frac{s_A}{k}\cdot(r_s-1)+1,\frac{s_A}{k}\cdot r_s]$, i.e., $v_s$ belongs to bin $r_s$   and  $i_{end}=C[r_e], v_e \in [\frac{s_A}{k}\cdot(r_e-1)+1,\frac{s_A}{k}\cdot r_e]$, i.e., $v_e$ belongs to bin $r_e$. We then run the program on this subset of records in $[i_{start},i_{end}]$. Note that for increased accuracy we can also consider preceding bins of $r_s-1$ for $i_{start}$ and succeeding bins of $r_e$ for $i_{end}$.



\begin{comment}
For answering queries of the form $\phi=A_1==v_1\wedge \ldots  \wedge A_n==v_n$, ideally we just need to compute for $A_2==v_2\wedge \ldots \wedge A_n==v_n$ on $ct_{A,v}$ number of records starting from position $\sum_{i=1}^{i=v-1}ct_{A,i}$ of $\boldsymbol{\mathcal{\tilde{D}}}_{sort}$. 

However the \textsf{AS} has access only to the noisy CDF over the $k$ bins $ct_{A,i}$. Note that when $\bar{i}_{start}=\bar{\hat{\mathcal{C}}}[v-1] < \sum_{i=1}^{i=v-1}ct_{A,i}$ and $\bar{i}_{end}= \bar{\hat{\mathcal{C}}}[v-1] > i_{start}+ct_{A,v}$, i.e., the indices computed from the noisy values  saddle over the true records satisfying $A==v$, then although we end up loosing in performance a bit, we are still guaranteed to compute the exact non-noisy count for records satisfying $\phi$. 

In all other cases, we end up disregarding some of the records that satisfy $A==v$, some of these rejected records in fact might additionally satisfy $A_1==v_1 \wedge \ldots \wedge A_n==v_n$. Thus we might get inaccurate answer for query predicate $\phi$ (note that here we are talking about the encrypted true count of the given query predicate that is computed by the AS via a series of transformations before applying the LaplaceMechanism primitive).  An effective heuristic to tackle this can be to compensate for the expected laplacian error as follows  $\bar{i}_{start}= \bar{\hat{\mathcal{C}}}[v-1]-\frac{2}{\epsilon}$ and $\bar{i}_{end}=\bar{\hat{\mathcal{C}}}[v]+\frac{2}{\epsilon}$. Also note that answering differentially private  range queries   on attribute $A$ can also be directly done from the noisy CDF $\bar{\hat{\mathcal{C}}}$ 
\end{comment}
 %\item GroupBy*($\mathbf{V},sk$)- This primitive is an extension of the previous GroupBy transformation. 
 

\subsection{DP Range Tree}
 Range queries constitute a very popular category of queries for typical databases and range trees are a popular data structure constructed to speed up range query answering. A 1-dimensional range tree for an attribute $A$ is an ordered data structure such that the leaf nodes correspond to the individual counts $ct_{A,i}$, $i$ increasing from left to right while the parent node stores the sum of the counts of its children. Hence an useful optimization for \system can be pre-computing the noisy range tree for some attributes. %In Crypt$\epsilon$ we construct a noisy range tree for some of the attribute.
 The sensitivity for each such noisy range tree is $\log k$ where $k$ is the domain size of the attribute. For answering any arbitrary range query, we need to access at most $2\log k$ nodes of the range tree. Thus to answer all possible range queries for the given attribute, the total squared error accumulated is $O(\frac{k^2(\log k)^2 }{\epsilon})$. In contrast for the naive case, we would have incurred error $O(\frac{k^3}{\epsilon})$. Hence this range tree optimization not only gives us a huge performance boost but also results in better answer accuracy. \\


\subsection{Precomputation}  
Recall that the data owners $\textsf{DO}_i$ send per-attribute encrypted one-hot-codings of their data. We can use the \textsf{CrossProduct} primitive to generate the one-hot-coding of data across two attributes at a time. However due to the intermediate interactions required with the \textsf{CSP}, the \textsf{CrossProduct} is a very costly operation. Hence an  useful optimization can be pre-computing the one-hot-coding for the data across a set of popular attributes $A^* \subset \mathcal{A}$ so that during subsequent program executions, the \textsf{AS} can get the desired representation via simple look-ups. 
%\paragraph*{\textbf{Optimized Crypt$\epsilon$ programs}}
%Let us reconsider the example programs covered in section 5. Both Program 1 and Program 2 can be optimized by constructing a range tree over attribute $Age$. Program  4 and Program 5  on the other hand can be improved by the differentially private index over attribute $NativeCountry$ while for Program 6 we can create the index over attribute $Gender$.

