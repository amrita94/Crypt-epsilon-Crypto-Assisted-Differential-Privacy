\section{\system Optimizations}\label{sec:optimization}
In this section we present several optimization techniques used by \system. These optimizations preserve differential privacy.

\subsection{Differential Privacy Based Optimizations}\label{sec:dp_optimization}
These optimizations leverage on the fact that the final output is  differentially private and expend the privacy budget.

\stitle{{\normalfont (1)} DP Indexing}: This optimization is motivated by the fact that the number of records needed by many programs is much smaller than the total dataset size.  For example, the program  P5 in Table~\ref{tab:programexamples} only needs records that have $Age=30$. With an index on $Age$, \system only need access a small subset of records instead of the entire dataset, but exact index leaks the exact data distribution. Hence, we will show how to build a basic differentially private index for a given attribute $A$ on encrypted database $\encD$.

%\\\textbf{1. Index} - Consider a conjunctive query predicate $A_1==v_1 \wedge A_2==v_2 \wedge A_3==v_3 $. If it so happens that the number of records satisfying $A_1==v_1$ is very low as compared to the total number of records, i.e., $ct_{A_1,v_1} << m$, then if a selection operation is performed alone on attribute $A_1$ then the size of the dataset to be considered for the subsequent clause $A_2==v_2 \wedge A_3==v_3$ reduces to only $ct_{A_1,v_1}$ as opposed to the whole dataset (size $m$). Our index optimization leverages on this idea - we create an index by sorting the database on an attribute of choice $A$. There are two heuristics that can be considered for selecting this indexing attribute $A$ - firstly $A$ should be a very frequently queried upon attribute. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Secondly if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,v_i}, i \in [n] << m$. This would ensure that the first selection operator performed alone on $A$ will filter out majority of the records and reduce the  dataset size to be considered for the subsequent predicate. 

We implement DP indexing via a garbled circuit which aims to compute two pieces of information: a sorted encrypted database based on $A$ and a $\epsilon$-differentially private index on $A$ (details in Appendix~\ref{app:index-imp}). The circuit outputs a sorted encrypted database and a differentially private historgram $\encV = [c_1,\ldots,c_k]$ for the $k$ bins. 
%\xh{May move the following to appendix} This circuit takes (i) the secret key $sk$ from the \CPS, and (ii) the entire database $\encD$ and the attribute $A$ from the \AS. The attribute $A$ has a domain of size $s_A$ and is uniformly partitioned into $k$ ranges $\{R_1,\ldots, R_i, \ldots, R_{s_A/k}\}$, where $R_i = [\frac{s_A}{k}i, \frac{s_A}{k}(i+1))$.  This circuit first decrypts $\encD$ using the secret key and sorts the decrypted database on attribute $A$ in ascending order. The sorted database is then encrypted again. Then the circuit computes a histogram $V$ on the $k$ ranges of $A$, denoted by $[c_1,\ldots,c_k]$  and perturbs each count with Laplace noise, i.e., $\hat{c}_i = c_i + \eta_i$, where $\eta_i\sim Lap(1/\epsilon)$. 
Then the \AS use the noisy counts $\encV$ to construct a cumulative histogram over $A$, where $\hat{cdf}[i] = \sum_{j=0}^i \hat{c}_i$ and post-processed such that they are in non-decreasing order and non-negative, and $\hat{cdf}[k] = |\encD|$~\cite{cdf}.  Given the differentially private cumulative histogram as the index and the sorted database, when a program would like to select rows with $A\in [v_i, v_j]$, we find the ranges that contain $v_i$ and $v_j$ respectively, denoted by $R_i$ and $R_j$. Then we return all records in the sorted database which cover all ranges from $R_i$ and $R_j$. This corresponds to row $(\hat{cdf}[i-1]+1)$ to row $(\hat{cdf}[j])$.

\eat{
Given an encrypted database $\encD$ and an attribute $A$ of $\encD$, we first use a garbled circuit to sort $\encD$ over $A$ into $\encD_s$ and then build a differentially private indexing on the sorted database $\encD_s$ (details in Appendix B.2). Next we build the index on $A$ with a privacy budget $\epsilon_{A}$ as follows. Let $P=(P_1,\ldots,P_k)$ be a uniform partition on the sorted domain of $A$ such that each partition contains $\frac{s_A}{k}$ consecutive domain values. We compute a noisy CDF $\hat{\mathcal{C}}$ vector $\hat{V}$ of length $k$ such that $\hat{V}[i]=\sum_j ct_{A,j}+\eta_i$ where $i \in [k], j \in P_i$ and $\eta_i\sim Lap(1/\epsilon_A)$. 
Next the \textsf{AS} computes a noisy CDF, $\hat{\mathcal{C}}$ over the $k$ bins using the noisy counts in $\hat{V}$ using inference based on isotonic regression \cite{cdf}. For executing a program conditioned as $\phi=A \in [v_s,v_e]$, we compute first compute $i_{start}=\hat{\mathcal{C}}[r_s-1], v_s \in [\frac{s_A}{k}\cdot(r_s-1)+1,\frac{s_A}{k}\cdot r_s]$, i.e., $v_s$ belongs to bin $r_s$   and  $i_{end}=\hat{\mathcal{C}}[r_e], v_e \in [\frac{s_A}{k}\cdot(r_e-1)+1,\frac{s_A}{k}\cdot r_e]$, i.e., $v_e$ belongs to bin $r_e$. We then run the program on this subset of records in $[i_{start},i_{end}]$. For increased accuracy we can also consider preceding bins of $r_s-1$ for $i_{start}$ and succeeding bins of $r_e$ for $i_{end}$.
}

\squishlist
\item \textbf{Optimized feature} - This optimization speeds up the execution time by reducing the number of total records to be processed for the program execution.
\item \textbf{Trade-off} - The trade-off is  a possible increase in error as the noisy selection of records from the index might miss some of the records that do satisfy the filter condition.
\item \textbf{Privacy Cost} - Assuming the index is constructed with a privacy parameter $\epsilon_A$, a selection of a subset of records using it will result in a $\epsilon_A$ - DP intermediate result. If $\epsilon$ is the parameter used for the subsequent measurement primitives, then by Theorem 1, the total privacy parameter is $\epsilon_A+\epsilon$.
\squishend

\xh{If space is an issue, say we discuss how to choose the attributes for indexing in appendix.} There are two heuristics that can be considered for selecting this indexing attribute $A$. First, $A$ should be very frequently queried. This is intuitive as this would mean a larger fraction of the queries will benefit from this optimization. Second, if $\{v_1,...v_n\} \subset dom(A)$ is the set of most frequently queried values for attribute $A$, then $ct_{A,i}, i \in [n] << m$ where $ct_{A,i}$ is the number of records in $\encD$ having value $v_i$ for attribute $A$. This would ensure that the first selection performed alone on $A$ will filter out majority of the records and reduce the intermediate dataset size to be considered for the subsequent predicate. 


\begin{comment}
For answering queries of the form $\phi=A_1==v_1\wedge \ldots  \wedge A_n==v_n$, ideally we just need to compute for $A_2==v_2\wedge \ldots \wedge A_n==v_n$ on $ct_{A,v}$ number of records starting from position $\sum_{i=1}^{i=v-1}ct_{A,i}$ of $\boldsymbol{\mathcal{\tilde{D}}}_{sort}$. 

However the \textsf{AS} has access only to the noisy CDF over the $k$ bins $ct_{A,i}$. Note that when $\bar{i}_{start}=\bar{\hat{\mathcal{C}}}[v-1] < \sum_{i=1}^{i=v-1}ct_{A,i}$ and $\bar{i}_{end}= \bar{\hat{\mathcal{C}}}[v-1] > i_{start}+ct_{A,v}$, i.e., the indices computed from the noisy values  saddle over the true records satisfying $A==v$, then although we end up loosing in performance a bit, we are still guaranteed to compute the exact non-noisy count for records satisfying $\phi$. 

In all other cases, we end up disregarding some of the records that satisfy $A==v$, some of these rejected records in fact might additionally satisfy $A_1==v_1 \wedge \ldots \wedge A_n==v_n$. Thus we might get inaccurate answer for query predicate $\phi$ (note that here we are talking about the encrypted true count of the given query predicate that is computed by the AS via a series of transformations before applying the LaplaceMechanism primitive).  An effective heuristic to tackle this can be to compensate for the expected laplacian error as follows  $\bar{i}_{start}= \bar{\hat{\mathcal{C}}}[v-1]-\frac{2}{\epsilon}$ and $\bar{i}_{end}=\bar{\hat{\mathcal{C}}}[v]+\frac{2}{\epsilon}$. Also note that answering differentially private  range queries   on attribute $A$ can also be directly done from the noisy CDF $\bar{\hat{\mathcal{C}}}$ 
\end{comment}



 
\stitle{{\normalfont (2)} DP Range Tree}:
Range queries constitute a very popular category of queries for typical databases and range trees are a popular data structure constructed to speed up range query answering \xh{add citation}. 
A 1-dimensional range tree for an attribute $A$ is an ordered hirearchical data structure such that the leaf nodes correspond to the individual counts for each possible value of $A$, while the parent node that cover a range of values stores the sum of the counts of its children. Hence, if range queries are common, pre-computing noisy range trees is an useful optimization, e.g. building a range tree on $Age$ attribute can be useful for programs P1 and P2 in Table~\ref{tab:programexamples}. The sensitivity for each such noisy range tree is $\log s_A$ where $s_A$ is the domain size of the attribute on which the tree is constructed. To answer any arbitrary range query, we need to access at most $2\log s_A$ number of nodes in the range tree. Thus to answer all possible range queries on $A$, the total squared error accumulated is $O(\frac{s^2(\log s_A)^2 }{\epsilon})$. In contrast for the naive case, we would have incurred error $O(\frac{s_A^3}{\epsilon})$. Hence this range tree optimization not only gives us a huge performance boost but also results in better answer accuracy.


%A 1-dimensional range tree for an attribute $A$ is an ordered data structure such that the leaf nodes correspond to the individual counts $ct_{A,i}$, $v_i \in dom(A)$ while each parent node stores the sum of the counts of its children. Hence an useful optimization for \system can be pre-computing the noisy range tree for some of the most popular attributes. This can be useful for programs P1 and P2 in Table 3. %In Crypt$\epsilon$ we construct a noisy range tree for some of the attribute.
%The sensitivity for each such noisy range tree is $\log k$ where $k$ is the domain size of the attribute on which the tree is constructed. For answering any arbitrary range query, we need to access at most $2\log k$ nodes of the range tree. Thus to answer all possible range queries for the given attribute, the total squared error accumulated is $O(\frac{k^2(\log k)^2 }{\epsilon})$. In contrast for the naive case, we would have incurred error $O(\frac{k^3}{\epsilon})$. Hence this range tree optimization not only gives us a huge performance boost but also results in better answer accuracy.



\squishlist
\item \textbf{Optimized Feature} - The optimization not only reduces the execution time, but also the expected error when executed over multiple range queries.
\item \textbf{Trade-off} - The trade-off for this optimization is the storage cost of the range tree $(O(2\cdot s_A))$.
\item \textbf{Privacy Cost} - If the range tree is constructed  with privacy parameter $\epsilon_R$, then any measurement on it is a post-processing step. Hence, the overal privacy cost is $\epsilon_R$-DP.
\squishend

\subsection{Implementation Based Optimizations} \label{sec:im_optimization}
These optimizations work completely on the encrypted data and do not expend any privacy budget. Hence they do not affect the accuracy of the executed programs.

\stitle{{\normalfont (1)} Precomputation}:  
Recall that the data owners $\textsf{DO}_i$ send per-attribute encrypted one-hot-codings of their data. We can use the \textsf{CrossProduct} primitive to generate the one-hot-coding of data across two attributes during program executions. However, this step is very cost due to the intermediate interactions required with the \textsf{CSP}. Hence, an  useful optimization can be  pre-computing the one-hot-coding for the data across a set of popular attributes $\bar{A} \subset \mathcal{A}$ so that during subsequent program executions, the \textsf{AS} can get the desired representation via simple look-ups.  This can be useful for example P3 in Table~\ref{tab:programexamples}.
\squishlist
\item \textbf{Optimized Feature} - This optimization improves the execution time of \system programs. These multi-attribute one-hot-encodings can be re-used for all subsequent programs.
%Moreover, once the multi-attribute one-hot-codings are created it can be re-used for all subsequent programs.
\item  \textbf{Trade-off} - The trade-off for this pre-computation is the storage cost (O($s_{\bar{A}}=\prod_{A \in \bar{A}}s_A$)) incurred to store the multi-attribute one-hot-codings for $\bar{A}$.
\item \textbf{Privacy Cost} - Since this computation is carried completely on the encrypted data, we do not expend any privacy budget.
\squishend

\stitle{{\normalfont (2)} Off-line Processing}:
In the implementation of $\textsf{GroupByCount}$ primitive, the \textsf{CSP} needs to generate the encrypted one-hot-codings for the masked histogram (Algorithm~\ref{groupby-imp}). Note that the one-hot-coding representation for any such count would simply be a vector of $(m-1)$ of ciphers for 0 $labEnc_{pk}(0)$ and and 1 cipher for 1 $labEnc_{pk}(1)$. Thus one useful optimization can be generating these ciphers for $0$ and $1$ in an off-line phase. (This is similar to the idea of off-line generation of Beaver's multiplication triples \cite{Beaver} used in secure multi-party computation.) In this way the encryption time will not be a part of the program execution time thereby giving us a performance boost.
\squishlist
\item \textbf{Optimized Feature} - This optimization results in a reduction in the run time of \system programs. 
\item \textbf{Trade-off} - The trade-off for this optimization is the storage cost (O($m\cdot s_A$)) incurred to store the ciphers for attribute $A$.
\item \textbf{Privacy Cost} - The computation is carried completely on the encrypted data, we do not expend any privacy budget.
\squishend

%\paragraph*{\textbf{Optimized Crypt$\epsilon$ programs}}
%Let us reconsider the example programs covered in section 5. Both Program 1 and Program 2 can be optimized by constructing a range tree over attribute $Age$. Program  4 and Program 5  on the other hand can be improved by the differentially private index over attribute $NativeCountry$ while for Program 6 we can create the index over attribute $Gender$.

